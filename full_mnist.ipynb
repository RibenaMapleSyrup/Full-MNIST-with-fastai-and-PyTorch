{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under the Hood: Training a Digit Classifier\n",
    "\n",
    "In this notebook I'll step through the process of training a very basic neural network digit classifier on the full mnist dataset using fastai. This task was the 'exam question' set at the end of Chapter 4 of the Deep Learning for Coders course where we began by training a digit classifier to differentiate images of '3's and '7's. \n",
    "\n",
    "I wanted to have a go at this as I've never really gone under the hood and put the work into understanding how all the individual steps of training a neural network hang together. I have studied linear algebra within an engineering context and applied AI within my day job for nearly two years yet I wouldn't know how to write a loss function. Whilst I am very familiar with much of the underlying maths, I still find the results of the process to be counter-intuitive. Its like a group of concepts than when taught well in isolation are very straightforward but when coupled remain slippery and surprising. Whenever I have tried to read deeper, I am greeted with unappealling looking formulas and ridiculously complicated sounding techniques that coupled together make deep learning kind-of unappealing to delve beyond application level. Furthmore, I can succesfully train a decent object detection model with several lines of PyTorch/Tensorflow and improve my model through following articles detailing higher level concepts- so why go deeper? Why go deep into training a simple model to do something boring like tell digits apart when I'm usually more interested in novel applications and creative computing? \n",
    "\n",
    "Well, in this case I've really enjoyed Deep Learning for Coders and found Jeremy to be excellent company. I also loved listening to a discussion Bruce Sterling recently had with Matt Dryhurst and Holly Herndon around AI art where Sterling suggested the most interesting aspect of this field is less the resultant artefact (some sculpture or GAN art) but more what's going on within the network as we train, likening it to kinetic art. So here is my mental mapping of that process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "!pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastai makes it very straightword to download MNIST alongside a load of other useful ML datasets. For the full dataset, folders are split into training (60,000 images) and testing (10,000 images). Each of those folders containers further folders named after the 10 different labels used within the dataset (numbers from 0-9).    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('training'),Path('testing')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.MNIST)\n",
    "Path.BASE_PATH = path\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = 0\n",
    "for root, dirs, files in os.walk(path):\n",
    "    images += len(files)\n",
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily access the images by storing lists of files within relevant variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = (path/'training'/'0').ls().sorted()\n",
    "ones = (path/'training'/'1').ls().sorted()\n",
    "twos = (path/'training'/'2').ls().sorted()\n",
    "threes = (path/'training'/'3').ls().sorted()\n",
    "fours = (path/'training'/'4').ls().sorted()\n",
    "fives = (path/'training'/'5').ls().sorted()\n",
    "sixes = (path/'training'/'6').ls().sorted()\n",
    "sevens = (path/'training'/'7').ls().sorted()\n",
    "eights = (path/'training'/'8').ls().sorted()\n",
    "nines = (path/'training'/'9').ls().sorted()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Python Imaging Library's Image class to open an example image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_zero = Image.open(zeros[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images are just represented as numbers within a computer, usually between values 0-255. We're working with grayscale images here so rather than requiring 3 or 4 values to describe each pixel, its just one value. To begin working with our images we pass them to our tensor function which will return a PyTorch tensor, a matrix similar to Numpy arrays although with some essential differences for deep learning. The image is stored within a tensor of shape 28,28. So 28 rows of 28 elements. The tensor has two dimensions so we say it has rank 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  51, 159, 253, 159,  50,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  48, 238, 252, 252, 252, 237,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  54, 227, 253, 252, 239, 233, 252,  57,   6,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,  10,  60, 224, 252, 253, 252, 202,  84, 252, 253, 122,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0, 163, 252, 252, 252, 253, 252, 252,  96, 189, 253, 167,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,  51, 238, 253, 253, 190, 114, 253, 228,  47,  79, 255, 168,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,  48, 238, 252, 252, 179,  12,  75, 121,  21,   0,   0, 253, 243,  50,   0,   0],\n",
       "        [  0,   0,   0,  38, 165, 253, 233, 208,  84,   0,   0,   0,   0,   0,   0, 253, 252, 165,   0,   0],\n",
       "        [  0,   0,   7, 178, 252, 240,  71,  19,  28,   0,   0,   0,   0,   0,   0, 253, 252, 195,   0,   0],\n",
       "        [  0,   0,  57, 252, 252,  63,   0,   0,   0,   0,   0,   0,   0,   0,   0, 253, 252, 195,   0,   0],\n",
       "        [  0,   0, 198, 253, 190,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 255, 253, 196,   0,   0],\n",
       "        [  0,  76, 246, 252, 112,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 253, 252, 148,   0,   0],\n",
       "        [  0,  85, 252, 230,  25,   0,   0,   0,   0,   0,   0,   0,   0,   7, 135, 253, 186,  12,   0,   0],\n",
       "        [  0,  85, 252, 223,   0,   0,   0,   0,   0,   0,   0,   0,   7, 131, 252, 225,  71,   0,   0,   0],\n",
       "        [  0,  85, 252, 145,   0,   0,   0,   0,   0,   0,   0,  48, 165, 252, 173,   0,   0,   0,   0,   0],\n",
       "        [  0,  86, 253, 225,   0,   0,   0,   0,   0,   0, 114, 238, 253, 162,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,  85, 252, 249, 146,  48,  29,  85, 178, 225, 253, 223, 167,  56,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,  85, 252, 252, 252, 229, 215, 252, 252, 252, 196, 130,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,  28, 199, 252, 252, 253, 252, 252, 233, 145,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,  25, 128, 252, 253, 252, 141,  37,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_tensor = tensor(first_zero)\n",
    "# portion of tensor\n",
    "first_tensor[0:25, 5:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor rank \n",
    "first_tensor.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this nice background feature within Pandas to visualise our image and values in better detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col0,#T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col1,#T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col2,#T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col3,#T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col4,#T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col5,#T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col6,#T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col7,#T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col8,#T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col9,#T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col10,#T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col11,#T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col12,#T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col13,#T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col14,#T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col15,#T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col16,#T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col17,#T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col18,#T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col19,#T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col0,#T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col1,#T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col2,#T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col3,#T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col4,#T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col5,#T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col6,#T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col7,#T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col8,#T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col9,#T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col10,#T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col11,#T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col12,#T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col13,#T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col14,#T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col15,#T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col16,#T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col17,#T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col18,#T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col19,#T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col0,#T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col1,#T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col2,#T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col3,#T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col4,#T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col5,#T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col6,#T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col7,#T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col8,#T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col9,#T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col10,#T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col11,#T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col12,#T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col13,#T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col14,#T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col15,#T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col16,#T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col17,#T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col18,#T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col19,#T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col0,#T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col1,#T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col2,#T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col3,#T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col4,#T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col5,#T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col6,#T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col7,#T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col8,#T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col9,#T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col10,#T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col11,#T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col12,#T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col13,#T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col14,#T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col15,#T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col16,#T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col17,#T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col18,#T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col19,#T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col0,#T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col1,#T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col2,#T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col3,#T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col4,#T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col5,#T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col6,#T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col7,#T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col8,#T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col9,#T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col15,#T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col16,#T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col17,#T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col18,#T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col19,#T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col0,#T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col1,#T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col2,#T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col3,#T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col4,#T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col5,#T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col6,#T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col7,#T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col8,#T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col15,#T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col16,#T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col17,#T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col18,#T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col19,#T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col0,#T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col1,#T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col2,#T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col3,#T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col4,#T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col5,#T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col6,#T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col7,#T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col17,#T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col18,#T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col19,#T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col0,#T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col1,#T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col2,#T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col3,#T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col4,#T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col5,#T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col17,#T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col18,#T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col19,#T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col0,#T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col1,#T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col2,#T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col3,#T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col4,#T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col5,#T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col17,#T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col18,#T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col19,#T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col0,#T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col1,#T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col2,#T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col3,#T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col4,#T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col17,#T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col18,#T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col19,#T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col0,#T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col1,#T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col2,#T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col3,#T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col13,#T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col14,#T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col18,#T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col19,#T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col0,#T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col1,#T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col2,#T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col9,#T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col10,#T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col11,#T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col12,#T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col13,#T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col14,#T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col18,#T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col19,#T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col0,#T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col1,#T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col9,#T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col10,#T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col11,#T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col12,#T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col13,#T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col14,#T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col18,#T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col19,#T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col0,#T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col1,#T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col6,#T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col7,#T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col8,#T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col9,#T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col10,#T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col11,#T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col12,#T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col13,#T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col14,#T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col18,#T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col19,#T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col0,#T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col1,#T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col5,#T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col6,#T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col7,#T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col8,#T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col9,#T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col10,#T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col11,#T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col12,#T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col13,#T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col14,#T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col18,#T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col19,#T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col0,#T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col5,#T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col6,#T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col7,#T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col8,#T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col9,#T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col10,#T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col11,#T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col12,#T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col13,#T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col14,#T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col18,#T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col19,#T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col0,#T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col5,#T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col6,#T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col7,#T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col8,#T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col9,#T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col10,#T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col11,#T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col12,#T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col18,#T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col19,#T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col0,#T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col4,#T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col5,#T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col6,#T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col7,#T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col8,#T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col9,#T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col10,#T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col11,#T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col17,#T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col18,#T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col19,#T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col0,#T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col4,#T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col5,#T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col6,#T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col7,#T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col8,#T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col9,#T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col10,#T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col15,#T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col16,#T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col17,#T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col18,#T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col19,#T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col0,#T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col4,#T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col5,#T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col6,#T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col7,#T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col8,#T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col9,#T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col14,#T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col15,#T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col16,#T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col17,#T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col18,#T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col19,#T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col0,#T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col14,#T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col15,#T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col16,#T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col17,#T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col18,#T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col19,#T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col0,#T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col12,#T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col13,#T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col14,#T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col15,#T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col16,#T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col17,#T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col18,#T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col19,#T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col0,#T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col10,#T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col11,#T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col12,#T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col13,#T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col14,#T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col15,#T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col16,#T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col17,#T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col18,#T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col19,#T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col0,#T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col1,#T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col9,#T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col10,#T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col11,#T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col12,#T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col13,#T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col14,#T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col15,#T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col16,#T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col17,#T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col18,#T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col19,#T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col0,#T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col1,#T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col2,#T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col3,#T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col4,#T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col5,#T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col6,#T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col7,#T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col8,#T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col9,#T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col10,#T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col11,#T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col12,#T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col13,#T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col14,#T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col15,#T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col16,#T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col17,#T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col18,#T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col19{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #ffffff;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col10,#T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col5{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #e2e2e2;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col11{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #727272;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col12,#T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col13,#T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col10,#T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col14,#T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col9,#T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col10,#T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col14,#T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col9,#T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col10,#T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col7,#T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col8,#T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col11,#T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col15,#T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col6,#T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col5,#T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col4,#T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col4,#T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col3,#T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col15,#T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col16,#T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col17,#T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col14,#T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col13,#T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col1,#T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col2,#T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col12,#T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col10,#T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col4,#T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col9,#T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col4,#T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col5,#T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col6,#T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col4,#T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col5,#T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col6{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #000000;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col13{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #717171;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col14{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #e3e3e3;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col9,#T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col4,#T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col11,#T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col5{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #e4e4e4;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col10,#T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col14,#T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col5,#T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col11{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #111111;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col11,#T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col12,#T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col11,#T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col11,#T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col7,#T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col8,#T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col11,#T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col12,#T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col7,#T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col16,#T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col16,#T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col17,#T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col3,#T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col16,#T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col17,#T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col3,#T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col16,#T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col2,#T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col2,#T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col2,#T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col2,#T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col2,#T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col3,#T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col7,#T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col8,#T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col3,#T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col7{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #010101;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col8{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #e0e0e0;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col9,#T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col12{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #1d1d1d;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col12,#T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col6{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #101010;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col13,#T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col6{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #161616;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col15,#T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col2{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #dedede;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col16,#T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col2,#T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col13,#T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col12{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #fcfcfc;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col6{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #fafafa;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col7{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #dcdcdc;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col8,#T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col1{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #222222;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col12{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #404040;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col13{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #c6c6c6;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col15,#T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col15,#T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col15,#T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col15,#T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col15,#T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col15,#T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col15,#T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col1,#T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col15,#T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col1,#T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col1,#T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col1,#T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col1{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #020202;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col16{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #9c9c9c;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col6{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #6d6d6d;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col13{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #bbbbbb;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col14{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #515151;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col16,#T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col12{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #6a6a6a;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col9,#T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col4,#T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col17{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #505050;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col10,#T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col10{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #a5a5a5;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col13{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #e5e5e5;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col14{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #cbcbcb;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col16{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #696969;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col8{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #5d5d5d;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col9{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #f9f9f9;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col10{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #cfcfcf;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col11{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #9d9d9d;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col12{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #f5f5f5;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col16{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #0c0c0c;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col17{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #d8d8d8;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col3,#T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col8{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #ececec;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col4{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #6b6b6b;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col7{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #383838;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col8{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #c7c7c7;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col17{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #303030;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col3,#T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col8{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #5e5e5e;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col5{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #0f0f0f;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col6{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #d2d2d2;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col7{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #f6f6f6;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col8{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #f2f2f2;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col5{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #dadada;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col2{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #464646;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col2{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #080808;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col4{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #a8a8a8;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col3{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #1b1b1b;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col4,#T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col2{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #f3f3f3;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col14{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #8c8c8c;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col16{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #555555;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col17{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #f8f8f8;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col3,#T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col15,#T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col11{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #232323;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col13{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #909090;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col16{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #d3d3d3;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col3{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #828282;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col12{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #6c6c6c;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col14{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #636363;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col3{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #212121;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col13{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #6e6e6e;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col3{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #050505;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col4{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #7f7f7f;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col6{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #f1f1f1;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col7{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #c5c5c5;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col9{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #1f1f1f;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col13{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #dfdfdf;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col5{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #1c1c1c;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col6{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #2c2c2c;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col10{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #484848;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col11{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #929292;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col1{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #c8c8c8;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col2{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #444444;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col8{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #171717;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col9{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #818181;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col3{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #949494;\n",
       "            color:  #000000;\n",
       "        }#T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col7{\n",
       "            font-size:  6pt;\n",
       "            background-color:  #868686;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >0</th>        <th class=\"col_heading level0 col1\" >1</th>        <th class=\"col_heading level0 col2\" >2</th>        <th class=\"col_heading level0 col3\" >3</th>        <th class=\"col_heading level0 col4\" >4</th>        <th class=\"col_heading level0 col5\" >5</th>        <th class=\"col_heading level0 col6\" >6</th>        <th class=\"col_heading level0 col7\" >7</th>        <th class=\"col_heading level0 col8\" >8</th>        <th class=\"col_heading level0 col9\" >9</th>        <th class=\"col_heading level0 col10\" >10</th>        <th class=\"col_heading level0 col11\" >11</th>        <th class=\"col_heading level0 col12\" >12</th>        <th class=\"col_heading level0 col13\" >13</th>        <th class=\"col_heading level0 col14\" >14</th>        <th class=\"col_heading level0 col15\" >15</th>        <th class=\"col_heading level0 col16\" >16</th>        <th class=\"col_heading level0 col17\" >17</th>        <th class=\"col_heading level0 col18\" >18</th>        <th class=\"col_heading level0 col19\" >19</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col3\" class=\"data row0 col3\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col4\" class=\"data row0 col4\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col5\" class=\"data row0 col5\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col6\" class=\"data row0 col6\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col7\" class=\"data row0 col7\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col8\" class=\"data row0 col8\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col9\" class=\"data row0 col9\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col10\" class=\"data row0 col10\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col11\" class=\"data row0 col11\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col12\" class=\"data row0 col12\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col13\" class=\"data row0 col13\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col14\" class=\"data row0 col14\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col15\" class=\"data row0 col15\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col16\" class=\"data row0 col16\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col17\" class=\"data row0 col17\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col18\" class=\"data row0 col18\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row0_col19\" class=\"data row0 col19\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col0\" class=\"data row1 col0\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col1\" class=\"data row1 col1\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col2\" class=\"data row1 col2\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col3\" class=\"data row1 col3\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col4\" class=\"data row1 col4\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col5\" class=\"data row1 col5\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col6\" class=\"data row1 col6\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col7\" class=\"data row1 col7\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col8\" class=\"data row1 col8\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col9\" class=\"data row1 col9\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col10\" class=\"data row1 col10\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col11\" class=\"data row1 col11\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col12\" class=\"data row1 col12\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col13\" class=\"data row1 col13\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col14\" class=\"data row1 col14\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col15\" class=\"data row1 col15\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col16\" class=\"data row1 col16\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col17\" class=\"data row1 col17\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col18\" class=\"data row1 col18\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row1_col19\" class=\"data row1 col19\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col0\" class=\"data row2 col0\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col1\" class=\"data row2 col1\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col2\" class=\"data row2 col2\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col3\" class=\"data row2 col3\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col4\" class=\"data row2 col4\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col5\" class=\"data row2 col5\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col6\" class=\"data row2 col6\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col7\" class=\"data row2 col7\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col8\" class=\"data row2 col8\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col9\" class=\"data row2 col9\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col10\" class=\"data row2 col10\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col11\" class=\"data row2 col11\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col12\" class=\"data row2 col12\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col13\" class=\"data row2 col13\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col14\" class=\"data row2 col14\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col15\" class=\"data row2 col15\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col16\" class=\"data row2 col16\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col17\" class=\"data row2 col17\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col18\" class=\"data row2 col18\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row2_col19\" class=\"data row2 col19\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col0\" class=\"data row3 col0\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col1\" class=\"data row3 col1\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col2\" class=\"data row3 col2\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col3\" class=\"data row3 col3\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col4\" class=\"data row3 col4\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col5\" class=\"data row3 col5\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col6\" class=\"data row3 col6\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col7\" class=\"data row3 col7\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col8\" class=\"data row3 col8\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col9\" class=\"data row3 col9\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col10\" class=\"data row3 col10\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col11\" class=\"data row3 col11\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col12\" class=\"data row3 col12\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col13\" class=\"data row3 col13\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col14\" class=\"data row3 col14\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col15\" class=\"data row3 col15\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col16\" class=\"data row3 col16\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col17\" class=\"data row3 col17\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col18\" class=\"data row3 col18\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row3_col19\" class=\"data row3 col19\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col0\" class=\"data row4 col0\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col1\" class=\"data row4 col1\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col2\" class=\"data row4 col2\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col3\" class=\"data row4 col3\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col4\" class=\"data row4 col4\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col5\" class=\"data row4 col5\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col6\" class=\"data row4 col6\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col7\" class=\"data row4 col7\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col8\" class=\"data row4 col8\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col9\" class=\"data row4 col9\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col10\" class=\"data row4 col10\" >51</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col11\" class=\"data row4 col11\" >159</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col12\" class=\"data row4 col12\" >253</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col13\" class=\"data row4 col13\" >159</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col14\" class=\"data row4 col14\" >50</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col15\" class=\"data row4 col15\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col16\" class=\"data row4 col16\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col17\" class=\"data row4 col17\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col18\" class=\"data row4 col18\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row4_col19\" class=\"data row4 col19\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col0\" class=\"data row5 col0\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col1\" class=\"data row5 col1\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col2\" class=\"data row5 col2\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col3\" class=\"data row5 col3\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col4\" class=\"data row5 col4\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col5\" class=\"data row5 col5\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col6\" class=\"data row5 col6\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col7\" class=\"data row5 col7\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col8\" class=\"data row5 col8\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col9\" class=\"data row5 col9\" >48</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col10\" class=\"data row5 col10\" >238</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col11\" class=\"data row5 col11\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col12\" class=\"data row5 col12\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col13\" class=\"data row5 col13\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col14\" class=\"data row5 col14\" >237</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col15\" class=\"data row5 col15\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col16\" class=\"data row5 col16\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col17\" class=\"data row5 col17\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col18\" class=\"data row5 col18\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row5_col19\" class=\"data row5 col19\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col0\" class=\"data row6 col0\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col1\" class=\"data row6 col1\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col2\" class=\"data row6 col2\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col3\" class=\"data row6 col3\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col4\" class=\"data row6 col4\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col5\" class=\"data row6 col5\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col6\" class=\"data row6 col6\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col7\" class=\"data row6 col7\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col8\" class=\"data row6 col8\" >54</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col9\" class=\"data row6 col9\" >227</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col10\" class=\"data row6 col10\" >253</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col11\" class=\"data row6 col11\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col12\" class=\"data row6 col12\" >239</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col13\" class=\"data row6 col13\" >233</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col14\" class=\"data row6 col14\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col15\" class=\"data row6 col15\" >57</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col16\" class=\"data row6 col16\" >6</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col17\" class=\"data row6 col17\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col18\" class=\"data row6 col18\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row6_col19\" class=\"data row6 col19\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col0\" class=\"data row7 col0\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col1\" class=\"data row7 col1\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col2\" class=\"data row7 col2\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col3\" class=\"data row7 col3\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col4\" class=\"data row7 col4\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col5\" class=\"data row7 col5\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col6\" class=\"data row7 col6\" >10</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col7\" class=\"data row7 col7\" >60</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col8\" class=\"data row7 col8\" >224</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col9\" class=\"data row7 col9\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col10\" class=\"data row7 col10\" >253</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col11\" class=\"data row7 col11\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col12\" class=\"data row7 col12\" >202</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col13\" class=\"data row7 col13\" >84</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col14\" class=\"data row7 col14\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col15\" class=\"data row7 col15\" >253</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col16\" class=\"data row7 col16\" >122</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col17\" class=\"data row7 col17\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col18\" class=\"data row7 col18\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row7_col19\" class=\"data row7 col19\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col0\" class=\"data row8 col0\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col1\" class=\"data row8 col1\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col2\" class=\"data row8 col2\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col3\" class=\"data row8 col3\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col4\" class=\"data row8 col4\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col5\" class=\"data row8 col5\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col6\" class=\"data row8 col6\" >163</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col7\" class=\"data row8 col7\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col8\" class=\"data row8 col8\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col9\" class=\"data row8 col9\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col10\" class=\"data row8 col10\" >253</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col11\" class=\"data row8 col11\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col12\" class=\"data row8 col12\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col13\" class=\"data row8 col13\" >96</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col14\" class=\"data row8 col14\" >189</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col15\" class=\"data row8 col15\" >253</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col16\" class=\"data row8 col16\" >167</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col17\" class=\"data row8 col17\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col18\" class=\"data row8 col18\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row8_col19\" class=\"data row8 col19\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col0\" class=\"data row9 col0\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col1\" class=\"data row9 col1\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col2\" class=\"data row9 col2\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col3\" class=\"data row9 col3\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col4\" class=\"data row9 col4\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col5\" class=\"data row9 col5\" >51</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col6\" class=\"data row9 col6\" >238</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col7\" class=\"data row9 col7\" >253</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col8\" class=\"data row9 col8\" >253</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col9\" class=\"data row9 col9\" >190</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col10\" class=\"data row9 col10\" >114</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col11\" class=\"data row9 col11\" >253</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col12\" class=\"data row9 col12\" >228</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col13\" class=\"data row9 col13\" >47</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col14\" class=\"data row9 col14\" >79</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col15\" class=\"data row9 col15\" >255</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col16\" class=\"data row9 col16\" >168</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col17\" class=\"data row9 col17\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col18\" class=\"data row9 col18\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row9_col19\" class=\"data row9 col19\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col0\" class=\"data row10 col0\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col1\" class=\"data row10 col1\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col2\" class=\"data row10 col2\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col3\" class=\"data row10 col3\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col4\" class=\"data row10 col4\" >48</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col5\" class=\"data row10 col5\" >238</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col6\" class=\"data row10 col6\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col7\" class=\"data row10 col7\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col8\" class=\"data row10 col8\" >179</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col9\" class=\"data row10 col9\" >12</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col10\" class=\"data row10 col10\" >75</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col11\" class=\"data row10 col11\" >121</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col12\" class=\"data row10 col12\" >21</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col13\" class=\"data row10 col13\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col14\" class=\"data row10 col14\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col15\" class=\"data row10 col15\" >253</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col16\" class=\"data row10 col16\" >243</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col17\" class=\"data row10 col17\" >50</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col18\" class=\"data row10 col18\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row10_col19\" class=\"data row10 col19\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col0\" class=\"data row11 col0\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col1\" class=\"data row11 col1\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col2\" class=\"data row11 col2\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col3\" class=\"data row11 col3\" >38</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col4\" class=\"data row11 col4\" >165</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col5\" class=\"data row11 col5\" >253</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col6\" class=\"data row11 col6\" >233</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col7\" class=\"data row11 col7\" >208</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col8\" class=\"data row11 col8\" >84</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col9\" class=\"data row11 col9\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col10\" class=\"data row11 col10\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col11\" class=\"data row11 col11\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col12\" class=\"data row11 col12\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col13\" class=\"data row11 col13\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col14\" class=\"data row11 col14\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col15\" class=\"data row11 col15\" >253</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col16\" class=\"data row11 col16\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col17\" class=\"data row11 col17\" >165</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col18\" class=\"data row11 col18\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row11_col19\" class=\"data row11 col19\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col0\" class=\"data row12 col0\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col1\" class=\"data row12 col1\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col2\" class=\"data row12 col2\" >7</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col3\" class=\"data row12 col3\" >178</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col4\" class=\"data row12 col4\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col5\" class=\"data row12 col5\" >240</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col6\" class=\"data row12 col6\" >71</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col7\" class=\"data row12 col7\" >19</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col8\" class=\"data row12 col8\" >28</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col9\" class=\"data row12 col9\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col10\" class=\"data row12 col10\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col11\" class=\"data row12 col11\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col12\" class=\"data row12 col12\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col13\" class=\"data row12 col13\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col14\" class=\"data row12 col14\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col15\" class=\"data row12 col15\" >253</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col16\" class=\"data row12 col16\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col17\" class=\"data row12 col17\" >195</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col18\" class=\"data row12 col18\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row12_col19\" class=\"data row12 col19\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col0\" class=\"data row13 col0\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col1\" class=\"data row13 col1\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col2\" class=\"data row13 col2\" >57</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col3\" class=\"data row13 col3\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col4\" class=\"data row13 col4\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col5\" class=\"data row13 col5\" >63</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col6\" class=\"data row13 col6\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col7\" class=\"data row13 col7\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col8\" class=\"data row13 col8\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col9\" class=\"data row13 col9\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col10\" class=\"data row13 col10\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col11\" class=\"data row13 col11\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col12\" class=\"data row13 col12\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col13\" class=\"data row13 col13\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col14\" class=\"data row13 col14\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col15\" class=\"data row13 col15\" >253</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col16\" class=\"data row13 col16\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col17\" class=\"data row13 col17\" >195</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col18\" class=\"data row13 col18\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row13_col19\" class=\"data row13 col19\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col0\" class=\"data row14 col0\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col1\" class=\"data row14 col1\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col2\" class=\"data row14 col2\" >198</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col3\" class=\"data row14 col3\" >253</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col4\" class=\"data row14 col4\" >190</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col5\" class=\"data row14 col5\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col6\" class=\"data row14 col6\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col7\" class=\"data row14 col7\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col8\" class=\"data row14 col8\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col9\" class=\"data row14 col9\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col10\" class=\"data row14 col10\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col11\" class=\"data row14 col11\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col12\" class=\"data row14 col12\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col13\" class=\"data row14 col13\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col14\" class=\"data row14 col14\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col15\" class=\"data row14 col15\" >255</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col16\" class=\"data row14 col16\" >253</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col17\" class=\"data row14 col17\" >196</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col18\" class=\"data row14 col18\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row14_col19\" class=\"data row14 col19\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col0\" class=\"data row15 col0\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col1\" class=\"data row15 col1\" >76</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col2\" class=\"data row15 col2\" >246</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col3\" class=\"data row15 col3\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col4\" class=\"data row15 col4\" >112</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col5\" class=\"data row15 col5\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col6\" class=\"data row15 col6\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col7\" class=\"data row15 col7\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col8\" class=\"data row15 col8\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col9\" class=\"data row15 col9\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col10\" class=\"data row15 col10\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col11\" class=\"data row15 col11\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col12\" class=\"data row15 col12\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col13\" class=\"data row15 col13\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col14\" class=\"data row15 col14\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col15\" class=\"data row15 col15\" >253</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col16\" class=\"data row15 col16\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col17\" class=\"data row15 col17\" >148</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col18\" class=\"data row15 col18\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row15_col19\" class=\"data row15 col19\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col0\" class=\"data row16 col0\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col1\" class=\"data row16 col1\" >85</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col2\" class=\"data row16 col2\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col3\" class=\"data row16 col3\" >230</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col4\" class=\"data row16 col4\" >25</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col5\" class=\"data row16 col5\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col6\" class=\"data row16 col6\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col7\" class=\"data row16 col7\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col8\" class=\"data row16 col8\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col9\" class=\"data row16 col9\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col10\" class=\"data row16 col10\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col11\" class=\"data row16 col11\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col12\" class=\"data row16 col12\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col13\" class=\"data row16 col13\" >7</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col14\" class=\"data row16 col14\" >135</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col15\" class=\"data row16 col15\" >253</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col16\" class=\"data row16 col16\" >186</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col17\" class=\"data row16 col17\" >12</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col18\" class=\"data row16 col18\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row16_col19\" class=\"data row16 col19\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col0\" class=\"data row17 col0\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col1\" class=\"data row17 col1\" >85</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col2\" class=\"data row17 col2\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col3\" class=\"data row17 col3\" >223</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col4\" class=\"data row17 col4\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col5\" class=\"data row17 col5\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col6\" class=\"data row17 col6\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col7\" class=\"data row17 col7\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col8\" class=\"data row17 col8\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col9\" class=\"data row17 col9\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col10\" class=\"data row17 col10\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col11\" class=\"data row17 col11\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col12\" class=\"data row17 col12\" >7</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col13\" class=\"data row17 col13\" >131</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col14\" class=\"data row17 col14\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col15\" class=\"data row17 col15\" >225</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col16\" class=\"data row17 col16\" >71</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col17\" class=\"data row17 col17\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col18\" class=\"data row17 col18\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row17_col19\" class=\"data row17 col19\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col0\" class=\"data row18 col0\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col1\" class=\"data row18 col1\" >85</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col2\" class=\"data row18 col2\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col3\" class=\"data row18 col3\" >145</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col4\" class=\"data row18 col4\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col5\" class=\"data row18 col5\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col6\" class=\"data row18 col6\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col7\" class=\"data row18 col7\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col8\" class=\"data row18 col8\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col9\" class=\"data row18 col9\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col10\" class=\"data row18 col10\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col11\" class=\"data row18 col11\" >48</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col12\" class=\"data row18 col12\" >165</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col13\" class=\"data row18 col13\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col14\" class=\"data row18 col14\" >173</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col15\" class=\"data row18 col15\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col16\" class=\"data row18 col16\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col17\" class=\"data row18 col17\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col18\" class=\"data row18 col18\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row18_col19\" class=\"data row18 col19\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col0\" class=\"data row19 col0\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col1\" class=\"data row19 col1\" >86</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col2\" class=\"data row19 col2\" >253</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col3\" class=\"data row19 col3\" >225</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col4\" class=\"data row19 col4\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col5\" class=\"data row19 col5\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col6\" class=\"data row19 col6\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col7\" class=\"data row19 col7\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col8\" class=\"data row19 col8\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col9\" class=\"data row19 col9\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col10\" class=\"data row19 col10\" >114</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col11\" class=\"data row19 col11\" >238</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col12\" class=\"data row19 col12\" >253</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col13\" class=\"data row19 col13\" >162</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col14\" class=\"data row19 col14\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col15\" class=\"data row19 col15\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col16\" class=\"data row19 col16\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col17\" class=\"data row19 col17\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col18\" class=\"data row19 col18\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row19_col19\" class=\"data row19 col19\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col0\" class=\"data row20 col0\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col1\" class=\"data row20 col1\" >85</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col2\" class=\"data row20 col2\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col3\" class=\"data row20 col3\" >249</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col4\" class=\"data row20 col4\" >146</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col5\" class=\"data row20 col5\" >48</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col6\" class=\"data row20 col6\" >29</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col7\" class=\"data row20 col7\" >85</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col8\" class=\"data row20 col8\" >178</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col9\" class=\"data row20 col9\" >225</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col10\" class=\"data row20 col10\" >253</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col11\" class=\"data row20 col11\" >223</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col12\" class=\"data row20 col12\" >167</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col13\" class=\"data row20 col13\" >56</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col14\" class=\"data row20 col14\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col15\" class=\"data row20 col15\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col16\" class=\"data row20 col16\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col17\" class=\"data row20 col17\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col18\" class=\"data row20 col18\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row20_col19\" class=\"data row20 col19\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col0\" class=\"data row21 col0\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col1\" class=\"data row21 col1\" >85</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col2\" class=\"data row21 col2\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col3\" class=\"data row21 col3\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col4\" class=\"data row21 col4\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col5\" class=\"data row21 col5\" >229</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col6\" class=\"data row21 col6\" >215</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col7\" class=\"data row21 col7\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col8\" class=\"data row21 col8\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col9\" class=\"data row21 col9\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col10\" class=\"data row21 col10\" >196</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col11\" class=\"data row21 col11\" >130</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col12\" class=\"data row21 col12\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col13\" class=\"data row21 col13\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col14\" class=\"data row21 col14\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col15\" class=\"data row21 col15\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col16\" class=\"data row21 col16\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col17\" class=\"data row21 col17\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col18\" class=\"data row21 col18\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row21_col19\" class=\"data row21 col19\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col0\" class=\"data row22 col0\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col1\" class=\"data row22 col1\" >28</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col2\" class=\"data row22 col2\" >199</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col3\" class=\"data row22 col3\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col4\" class=\"data row22 col4\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col5\" class=\"data row22 col5\" >253</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col6\" class=\"data row22 col6\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col7\" class=\"data row22 col7\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col8\" class=\"data row22 col8\" >233</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col9\" class=\"data row22 col9\" >145</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col10\" class=\"data row22 col10\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col11\" class=\"data row22 col11\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col12\" class=\"data row22 col12\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col13\" class=\"data row22 col13\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col14\" class=\"data row22 col14\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col15\" class=\"data row22 col15\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col16\" class=\"data row22 col16\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col17\" class=\"data row22 col17\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col18\" class=\"data row22 col18\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row22_col19\" class=\"data row22 col19\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col0\" class=\"data row23 col0\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col1\" class=\"data row23 col1\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col2\" class=\"data row23 col2\" >25</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col3\" class=\"data row23 col3\" >128</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col4\" class=\"data row23 col4\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col5\" class=\"data row23 col5\" >253</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col6\" class=\"data row23 col6\" >252</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col7\" class=\"data row23 col7\" >141</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col8\" class=\"data row23 col8\" >37</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col9\" class=\"data row23 col9\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col10\" class=\"data row23 col10\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col11\" class=\"data row23 col11\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col12\" class=\"data row23 col12\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col13\" class=\"data row23 col13\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col14\" class=\"data row23 col14\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col15\" class=\"data row23 col15\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col16\" class=\"data row23 col16\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col17\" class=\"data row23 col17\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col18\" class=\"data row23 col18\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row23_col19\" class=\"data row23 col19\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col0\" class=\"data row24 col0\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col1\" class=\"data row24 col1\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col2\" class=\"data row24 col2\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col3\" class=\"data row24 col3\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col4\" class=\"data row24 col4\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col5\" class=\"data row24 col5\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col6\" class=\"data row24 col6\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col7\" class=\"data row24 col7\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col8\" class=\"data row24 col8\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col9\" class=\"data row24 col9\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col10\" class=\"data row24 col10\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col11\" class=\"data row24 col11\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col12\" class=\"data row24 col12\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col13\" class=\"data row24 col13\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col14\" class=\"data row24 col14\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col15\" class=\"data row24 col15\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col16\" class=\"data row24 col16\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col17\" class=\"data row24 col17\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col18\" class=\"data row24 col18\" >0</td>\n",
       "                        <td id=\"T_06fa0214_0c63_11eb_8bce_0242ac110002row24_col19\" class=\"data row24 col19\" >0</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f50bbcd37c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(first_tensor[0:25, 5:25])\n",
    "df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Try: Pixel Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we build our fancy neural network model we should begin by creating a simple baseline model with which we can access the performance of any subsequent models. This baseline should be simple to implement and test. \n",
    "\n",
    "For a baseline model we will first find the average pixel value for every pixel across all images within each digit class. This will give us ten tensors containing average pixel values, defining what we might call the \"ideal\" image for each digit class or the average image for each class. Then, to classify a new image, we see which of our average images our new image is most similar to. This is simplistic so should make a good baseline.\n",
    "\n",
    "First step is creating our average image for each class. We'll do this by stacking all our images for each class within a set of tensors. This can take a while, so maybe time to make tea. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero\n",
      "one\n",
      "two\n",
      "three\n",
      "four\n",
      "five\n",
      "six\n",
      "seven\n",
      "eight\n",
      "nine\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6131, 6265, 5949)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_tensors = [tensor(Image.open(o)) for o in zeros]\n",
    "print('zero')\n",
    "one_tensors = [tensor(Image.open(o)) for o in ones]\n",
    "print('one')\n",
    "two_tensors = [tensor(Image.open(o)) for o in twos]\n",
    "print('two')\n",
    "three_tensors = [tensor(Image.open(o)) for o in threes]\n",
    "print('three')\n",
    "four_tensors = [tensor(Image.open(o)) for o in fours]\n",
    "print('four')\n",
    "five_tensors = [tensor(Image.open(o)) for o in fives]\n",
    "print('five')\n",
    "six_tensors = [tensor(Image.open(o)) for o in sixes]\n",
    "print('six')\n",
    "seven_tensors = [tensor(Image.open(o)) for o in sevens]\n",
    "print('seven')\n",
    "eight_tensors = [tensor(Image.open(o)) for o in eights]\n",
    "print('eight')\n",
    "nine_tensors = [tensor(Image.open(o)) for o in nines]\n",
    "print('nine')\n",
    "len(three_tensors),len(seven_tensors), len(nine_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = [zero_tensors, one_tensors, two_tensors, three_tensors, four_tensors, five_tensors, six_tensors, seven_tensors, eight_tensors, nine_tensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(three_tensors[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every pixel position, we want to compute the average over all the images of the intensity of that pixel. To do this we first combine all the images in this list into a single three-dimensional tensor. The most common way to describe such a tensor is to call it a rank-3 tensor. We often need to stack up individual tensors in a collection into a single tensor, like a really tall thin skyscraper. Unsurprisingly, PyTorch comes with a function called stack that we can use for this purpose.\n",
    "\n",
    "Some operations in PyTorch, such as taking a mean, require us to cast our integer types to float types. Since we'll be needing this later, we'll also cast our stacked tensor to float now. Casting in PyTorch is as simple as typing the name of the type you wish to cast to, and treating it as a method.\n",
    "\n",
    "Generally in Computer Vision when images are floats, the pixel values are expected to be between 0 and 1, so we will also divide by 255 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_zeros = torch.stack(zero_tensors).float()/255\n",
    "stacked_ones = torch.stack(one_tensors).float()/255\n",
    "stacked_twos = torch.stack(two_tensors).float()/255\n",
    "stacked_threes = torch.stack(three_tensors).float()/255\n",
    "stacked_fours = torch.stack(four_tensors).float()/255\n",
    "stacked_fives = torch.stack(five_tensors).float()/255\n",
    "stacked_sixes = torch.stack(six_tensors).float()/255\n",
    "stacked_sevens = torch.stack(seven_tensors).float()/255\n",
    "stacked_eights = torch.stack(eight_tensors).float()/255\n",
    "stacked_nines = torch.stack(nine_tensors).float()/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute what our ideal digits looks like by taking the mean along dimension 0 of each stacked, rank-3 tensor. This is the dimension that indexes over all the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean0 = stacked_zeros.mean(0)\n",
    "mean1 = stacked_ones.mean(0)\n",
    "mean2 = stacked_twos.mean(0)\n",
    "mean3 = stacked_threes.mean(0)\n",
    "mean4 = stacked_fours.mean(0)\n",
    "mean5 = stacked_fives.mean(0)\n",
    "mean6 = stacked_sixes.mean(0)\n",
    "mean7 = stacked_sevens.mean(0)\n",
    "mean8 = stacked_eights.mean(0)\n",
    "mean9 = stacked_nines.mean(0)\n",
    "show_image(mean4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the mean of a stack of tensors representing images can be quite interesting. I tried this for a dataset of bears. From this we can deduce the likely position and colour of our average bear as well as the presence of green foliage around the bear. Its quite a nice foggy aesthetic and I would be curious to explore this technique further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/average_bear.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# silly example\n",
    "# pix = Path('bears/black').ls().sorted()\n",
    "# bears = [tensor(Image.open(bear).resize((300,300)).convert('RGB')) for bear in pix]\n",
    "# stacked_bears = torch.stack(bears).float()/255\n",
    "# meanBear = stacked_bears.mean(0)\n",
    "# show_image(meanBear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've created our mean images lets find the similarity between our mean and a new image by calculating the distance between them. To do this we use two methods, RMSE (root mean squared error) and L1 norm (absolute difference value) for calculating distance between sets of values where directionality is ignored (ie whether difference is a positive or negative number). We use these methods because we want to calculate the magnitude of distance between values and the squared terms remove directionality. Both approaches are valid for this problem although if we're getting into it, MSE differs in that it will penalize bigger mistakes more heavily than L1 although it will be more lenient with small mistakes. We manually implement these methods here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# new image\n",
    "a_3 = stacked_threes[1]\n",
    "show_image(a_3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1114), tensor(0.2021))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare an image of 3 to our mean 3\n",
    "dist_3_abs = (a_3 - mean3).abs().mean()\n",
    "dist_3_sqr = ((a_3 - mean3)**2).mean().sqrt()\n",
    "dist_3_abs,dist_3_sqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1586), tensor(0.3021))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare an image of 3 to our mean 7 \n",
    "dist_7_abs = (a_3 - mean7).abs().mean()\n",
    "dist_7_sqr = ((a_3 - mean7)**2).mean().sqrt()\n",
    "dist_7_abs,dist_7_sqr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch includes both these functions as 'loss functions' ie methods for calculating the magnitude of the difference or 'distance' between our expected result and our predicted result as a single number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1586), tensor(0.3021))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# l1 norm is the absolute value of differences \n",
    "# mse - mean squared error\n",
    "F.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy Arrays vs PyTorch Tensors (aside)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go further, why are we using PyTorch tensors rather than Numpy arrays? \n",
    "\n",
    "So PyTorch tensors and Numpy arrays have very similar APIs although key differences are, Numpy doesn't support using the GPU or calculating gradients. These are two things we really need for deep learning. It would basically be impossible to do deep learning without these capabilities.\n",
    "\n",
    "PyTorch tensors can live on the GPU where their computation is optimized for the GPU and as such runs a lot faster given enough values to work with. Most importantly we can automatically calculate derivatives of our operations (gradients) and also combinations of operations. \n",
    "\n",
    "Python is slow, but Numpy and PyTorch are much faster. However for performing algebriac operations on matrices, PyTorch demonstrates far superior performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1713268756866455"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch \n",
    "x = torch.rand(10000, 10000)\n",
    "y = torch.rand(10000, 10000)\n",
    "\n",
    "current_time = time.time()\n",
    "\n",
    "a = x + y\n",
    "b = x - y\n",
    "c = x * y\n",
    "\n",
    "end_time = time.time()\n",
    "end_time - current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2605297565460205"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy\n",
    "x = np.random.rand(10000, 10000)\n",
    "y = np.random.rand(10000, 10000)\n",
    "\n",
    "current_time = time.time()\n",
    "\n",
    "a = x + y\n",
    "b = x - y\n",
    "c = x * y\n",
    "\n",
    "end_time = time.time()\n",
    "end_time - current_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Metrics Using Broadcasting\n",
    "\n",
    "Metrics are human-friendly numbers that tell us how well our model is performing (ie accuracy of 73% rather than something less friendly like RMSE=0.1586). We calculate metrics based on the data our model has not seen to avoid overfitting. That data sits in testing so lets extract it now: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_0_tens = torch.stack([tensor(Image.open(o)) \n",
    "                            for o in (path/'testing'/'0').ls()])\n",
    "valid_0_tens = valid_0_tens.float()/255\n",
    "valid_1_tens = torch.stack([tensor(Image.open(o)) \n",
    "                            for o in (path/'testing'/'1').ls()])\n",
    "valid_1_tens = valid_1_tens.float()/255\n",
    "valid_2_tens = torch.stack([tensor(Image.open(o)) \n",
    "                            for o in (path/'testing'/'2').ls()])\n",
    "valid_2_tens = valid_2_tens.float()/255\n",
    "valid_3_tens = torch.stack([tensor(Image.open(o)) \n",
    "                            for o in (path/'testing'/'3').ls()])\n",
    "valid_3_tens = valid_3_tens.float()/255\n",
    "valid_4_tens = torch.stack([tensor(Image.open(o)) \n",
    "                            for o in (path/'testing'/'4').ls()])\n",
    "valid_4_tens = valid_4_tens.float()/255\n",
    "valid_5_tens = torch.stack([tensor(Image.open(o)) \n",
    "                            for o in (path/'testing'/'5').ls()])\n",
    "valid_5_tens = valid_5_tens.float()/255\n",
    "valid_6_tens = torch.stack([tensor(Image.open(o)) \n",
    "                            for o in (path/'testing'/'6').ls()])\n",
    "valid_6_tens = valid_6_tens.float()/255\n",
    "valid_7_tens = torch.stack([tensor(Image.open(o)) \n",
    "                            for o in (path/'testing'/'7').ls()])\n",
    "valid_7_tens = valid_7_tens.float()/255\n",
    "valid_8_tens = torch.stack([tensor(Image.open(o)) \n",
    "                            for o in (path/'testing'/'8').ls()])\n",
    "valid_8_tens = valid_8_tens.float()/255\n",
    "valid_9_tens = torch.stack([tensor(Image.open(o)) \n",
    "                            for o in (path/'testing'/'9').ls()])\n",
    "valid_9_tens = valid_9_tens.float()/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check our tensor's shape to check all is well. As we expect we see 1010 images of shape 28x28 have been stacked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1010, 28, 28])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_3_tens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to do now is to calculate distance between an image within a validation tensor with a mean image to determine the distance between them. \n",
    "\n",
    "To do this, we first subtract both tensors, take absolute value of the resulting tensor of difference values and then sum all values to get the mean pixel distance between both tensors as described within the below function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_distance(a,b): return (a-b).abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try it out with our example image (a_3) and all of the mean images to find which it is closest to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [mean0, mean1, mean2, mean3, mean4, mean5, mean6, mean7, mean8, mean9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1879)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distance between image of a 3 and average 0 image:\n",
    "distance = mnist_distance(a_3, mean0)\n",
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def what_is(image):\n",
    "    similar_mean = 0\n",
    "    distance = mnist_distance(image, mean0)\n",
    "    for i in range(len(means)):\n",
    "        if mnist_distance(image, means[i]) < distance:\n",
    "            similar_mean = i\n",
    "            distance = mnist_distance(image, means[i])\n",
    "    print('most similar image: ' + str(similar_mean))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give it a go with a range of tensors ie one_tensors[2] or seven_tensors[8]. If you play with it enough you'll realise it works ok but its by no means perfect! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most similar image: 7\n"
     ]
    }
   ],
   "source": [
    "what_is(seven_tensors[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, what if we wanted to calculate the accuracy of the model against the thousands of images within each class? \n",
    "For this we would usually have to loop through all the images within each class, passing them to the function one by \n",
    "one and then stacking the results in a tensor to calculate the accuracy. We would be calculating distance between our mean tensor of rank 2 and each rank 2 tensor within our rank 3 tensor containing all of the images for each class. However looping through all these images is inefficient and Python has a really nice feature called broadcasting that we can exploit to perform operations on matrices of different rank. \n",
    "\n",
    "Broadcasting works as follows: if we are performing operations on two matrices of different rank, Python automatically expand the tensor of smaller rank to have the same size as the larger tensor and allow us to easily perform our operation. This works great here in that we can instantly calculate the prediction for each image within our larger validation tensors without looping! The only thing we need to add to our function is a specification of what tensor dimensions we want to calculate our mean value as we are now working with a rank 3 tensor rather than rank 2 and PyTorch just stores the data as numbers in memory- the framework doesn't recognise the tensor as a stack of images, just a tensor of numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine our function\n",
    "def mnist_distance(a,b): return ((a-b).abs()).mean((-1,-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [mean0, mean1, mean2, mean3, mean4, mean5, mean6, mean7, mean8, mean9]\n",
    "validations = [valid_0_tens, valid_1_tens, valid_2_tens, valid_3_tens, valid_4_tens, valid_5_tens, valid_6_tens, valid_7_tens, valid_8_tens, valid_9_tens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_predictions(x, label): \n",
    "    previous_distance = mnist_distance(x, mean0)\n",
    "    previous_predictions = torch.zeros([len(x)]).int()\n",
    "    for j in range(len(means[1:])): \n",
    "        current_distance = mnist_distance(x, means[j+1])\n",
    "        previous_predictions = torch.where(current_distance < previous_distance, tensor(j+1).int(), previous_predictions)\n",
    "        previous_distance = torch.where(current_distance < previous_distance, current_distance, previous_distance)\n",
    "    histogram = torch.histc(previous_predictions.float(), bins=10, min=0, max=9)\n",
    "    mode_value = torch.argmax(histogram).item()\n",
    "    correct_predictions = histogram[label].int().item()\n",
    "    accuracy = histogram[label].int().item()/len(x)\n",
    "    print(\"predicted class: \" + str(mode_value) + \", accuracy: \" + str(accuracy))\n",
    "    return histogram, correct_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this function to plot histograms of all our models predictions for each validation set. We notice that 1's are classified with best accuracy by far. This feels intuitive as 1 is likely to be the most simple digit representing a line... zeros are great, however other classes perform more poorly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class: 0, accuracy: 0.8153061224489796\n"
     ]
    }
   ],
   "source": [
    "# torch.histc(mean_predictions(valid_0_tens))\n",
    "h, c = mean_predictions(valid_0_tens, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall accuracy of our model across the complete validation dataset is 66.85%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class: 0, accuracy: 0.8153061224489796\n",
      "predicted class: 1, accuracy: 0.9982378854625551\n",
      "predicted class: 1, accuracy: 0.42344961240310075\n",
      "predicted class: 3, accuracy: 0.6089108910891089\n",
      "predicted class: 4, accuracy: 0.6680244399185336\n",
      "predicted class: 1, accuracy: 0.32623318385650224\n",
      "predicted class: 6, accuracy: 0.7870563674321504\n",
      "predicted class: 7, accuracy: 0.7645914396887159\n",
      "predicted class: 8, accuracy: 0.44250513347022585\n",
      "predicted class: 9, accuracy: 0.77601585728444\n",
      "model accuracy: 0.6685\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0 \n",
    "length = 0\n",
    "for i in range(len(validations)):\n",
    "    h,c = mean_predictions(validations[i], i)\n",
    "    correct_predictions += c\n",
    "    length += len(validations[i])\n",
    "print(\"model accuracy: \" + str(correct_predictions/length)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we improve our model? We need to have some way of testing the effectiveness of our current weights in terms of performance and implementing a way of adjusting weights to maximise performance (in Arthur Samuel's words). This is how our model will get better and better. \n",
    "\n",
    "We're missing two things: a kind of weight assignment and a way of improving our weights by testing them.\n",
    "\n",
    "So lets first of all give each pixel within our images a set of weights (w) such that when we multiply our pixels with these weights, larger weights are associated with pixels more likely to be black for our particular category. This is kindof similar to what we saw previously with pixel similarity where we expect less distance when the pixels we are comparing are similar- only here we're inverting this and saying we want to return higher values when pixels are more likely to correspond to the class the image they belong to sits within. We can express this as a function that we expect to return relatively larger values when provided with an image (x) corresponding to the class it belongs to. ie we want this function to return highest value when x is an image of an 8.\n",
    "\n",
    "def probability_image_is_an_eight(x,w) = (x*w).sum()\n",
    "\n",
    "So to actually improve our weights tensor (w) such that it returns high value and allows us to differentiate our images to a fantastic level of accuracy (relative to our baseline) we need to follow the following 7 steps that underpin all training of deep learning models. \n",
    "\n",
    "1. initialize weights - here we usually start with random values unless we are transfer learning then we start with some pretrained weights. \n",
    "2. use weights to predict image class\n",
    "3. calculate how good our model is in terms of loss - remember loss needs to decrease when our network's performance improves and it also needs to be sensitive enought to change when individual weights are altered such that we can calculate the gradient of our weights and understand the outcome of altering that weight assignment on the model performance\n",
    "4. calculate the gradient for each of the many many weights in our network (how loss changes relative to change in JUST THAT SINGLE WEIGHT)\n",
    "5. change all the weights based on our gradient calculations (this is called our Step)\n",
    "6. Go back to step 2 and repeat the whole process\n",
    "7. Iterate until we're happy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be a little confusing at first in terms of syntax but its actually really powerful. We can easily calculate the gradient of our tensors by using the require_grad method. \n",
    "\n",
    "For instance: \n",
    "xt = tensor(3.).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretend this is our loss function and x is our weight \n",
    "# we want to work out the impact of adjusting our weight on the loss function\n",
    "def f(x): return x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight we are investigating has the value 3 and we want to figure out whether we should increase or descrease it. We have to calculate the gradient first to figure our what action will decrease our loss. We use the requires_grad method to specify that we want to calculate the gradient of this weight. Gradient usually means the slope in maths but here we mean a value ie gradient = -0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tensor(3.).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = f(weight)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So loss = 9 ... how do we reduce this? First we need to calculate the gradient. In deep learning we have a fancy complicated word for this: we need to 'backpropogate' ie for each layer of our neural network, calculate the gradient for each weight, or in simpler terms, how much will our loss change when we change our weight by a certain amount (whilst keeping all other weights constant)? Lets backpropagate! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check the gradient for our particular weight assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, its the derivate of x^2, 2x with our value 3 subbed in for x. Its incredibly simple. The syntax may be confusing at first but its doing something very basic over and over again. Importantly we can do this for whole tensors of values using PyTorch. So now lets step and use the learning parameter to adjust our weight value. There is a lot to say about learning rate but for now, if we go to small it will take forever to train our model and we can stuck within local minima. If we go too high with our learning rate it will bounce around without ever diverging. There are techniques for mitigating this but that's a whole other topic.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9400, requires_grad=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.01\n",
    "weight.data -= lr*weight.grad.data\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.6436, grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've successfully reduced our loss by 0.367!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll attempt to write a classifier for full MNIST. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll store all of our training images within a single tensor and change them from a list of matrices to a rank 2 tensor. So basically stacking 60,000 flattened (1D) images on top of eachother. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.cat([stacked_zeros, stacked_ones, stacked_twos, stacked_threes, stacked_fours, stacked_fives, stacked_sixes, stacked_sevens, stacked_eights, stacked_nines]).view(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 784])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a similar tensor containing all of the labels for each image within our \n",
    "training dataset. We want to classify our images as one of 10 digits. So our classifiers output should communicate one of 10 numbers. Therefore we are trying to reduce an 28x28 shaped tensor representing our image to a single number representing each class. Its tempting to initially think we should use the label as the classification output ie all zero images should be labelled as tensor(0). However this approach would not allow us to calculate the propabilities our image belongs to each class because its only outputting one number between 0-9. What would be better would be an output of 10 numbers with each indicating the probability that the input image belongs to the corresponding class. For example a zero image should return: \n",
    "\n",
    "tensor([1,0,0,0,0,0,0,0,0,0]) \n",
    "\n",
    "This approach to labelling is called One-Hot Encoding and is the approach we'll take here. Its also standard for multi-class classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = tensor([[1,0,0,0,0,0,0,0,0,0]]*len(zeros)+ \n",
    "           [[0,1,0,0,0,0,0,0,0,0]]*len(ones)+ \n",
    "           [[0,0,1,0,0,0,0,0,0,0]]*len(twos)+\n",
    "           [[0,0,0,1,0,0,0,0,0,0]]*len(threes)+\n",
    "           [[0,0,0,0,1,0,0,0,0,0]]*len(fours)+\n",
    "           [[0,0,0,0,0,1,0,0,0,0]]*len(fives)+\n",
    "           [[0,0,0,0,0,0,1,0,0,0]]*len(sixes)+\n",
    "           [[0,0,0,0,0,0,0,1,0,0]]*len(sevens)+\n",
    "           [[0,0,0,0,0,0,0,0,1,0]]*len(eights)+\n",
    "           [[0,0,0,0,0,0,0,0,0,1]]*len(nines)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example zero image label: \n",
    "train_y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch requires us to create a Dataset to return a tuple of (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = list(zip(train_x,train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = dset[0]\n",
    "x.shape,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a DataLoader from our dataset which will pass our model our data in batches. We use batches because we need to periodically calculate loss on our dataset as we train. One image wouldn't provide us with much useful information (provide a very unstable and imprecise gradient) and our whole dataset would take forever so we compromise and choose a number in between to calculate our average loss across some images. Batch size impacts speed and accuracy of training and picking a good one is a hot topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 784]), torch.Size([1024, 10]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoader(dset, batch_size=1024, shuffle=True)\n",
    "xb,yb = first(dl)\n",
    "xb.shape,yb.shape\n",
    "# first mini batch containes our images (256 rows, 784 long (28x28 flattened image)) and labels (265)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets repeat this process for our validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "validations = [valid_0_tens, valid_1_tens, valid_2_tens, valid_3_tens, valid_4_tens, valid_5_tens, valid_6_tens, valid_7_tens, valid_8_tens, valid_9_tens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = torch.cat(validations).view(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y = tensor([[1,0,0,0,0,0,0,0,0,0]]*len(valid_0_tens)+ \n",
    "           [[0,1,0,0,0,0,0,0,0,0]]*len(valid_1_tens)+ \n",
    "           [[0,0,1,0,0,0,0,0,0,0]]*len(valid_2_tens)+\n",
    "           [[0,0,0,1,0,0,0,0,0,0]]*len(valid_3_tens)+\n",
    "           [[0,0,0,0,1,0,0,0,0,0]]*len(valid_4_tens)+\n",
    "           [[0,0,0,0,0,1,0,0,0,0]]*len(valid_5_tens)+\n",
    "           [[0,0,0,0,0,0,1,0,0,0]]*len(valid_6_tens)+\n",
    "           [[0,0,0,0,0,0,0,1,0,0]]*len(valid_7_tens)+\n",
    "           [[0,0,0,0,0,0,0,0,1,0]]*len(valid_8_tens)+\n",
    "           [[0,0,0,0,0,0,0,0,0,1]]*len(valid_9_tens))\n",
    "\n",
    "valid_dset = list(zip(valid_x,valid_y))\n",
    "\n",
    "valid_dl = DataLoader(valid_dset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) INITIALIZE\n",
    "\n",
    "First we need a function to initialize our weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our weights we'll use the most basic possible configuration for the problem - 10 28x28 tensors to reduce our input image to a 10x1 shape tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = init_params((10,28*28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to add a bias term as we encounter 0 gradients when working with 0 pixels for any value of weights. This is very unflexible abd will hinder our progress in improving our model. So a random bias tensor can help.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = init_params(10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 784]), torch.Size([10]))"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape, bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) PREDICT \n",
    "\n",
    "Here we define our linear model. Our simple model multiplies all our images by our weights and add bias term to return a prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear1(xb): return xb@weights.T + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it returns an array of random numbers which we can then use to calculate loss. Lets define a loss function. We can see our predictions have a wide range of values and we're looking to compare them with our label tensors which lie within the probability range of 0-1 and also sum to 1 so we need to transform our predictions to this format. We can apply PyTorch's softmax function to transform our predictions into a tensor of this form that we can use to calculate loss.\n",
    "\n",
    "However now we need to figure out the best way of calculating loss! I tried one way initially similar to how we improve a model with binary classification. However it wasn't super successful and never really trained a model beyond 65% accuracy. For lower learning rates it would often get stuck. From the literature around multi-class classification, it seems cross-entropy loss is standard so I found a nice friendly article explaning how to implement it and replicated here. It trained a significantly more accurate model! But feel free to sub either loss to see the difference a suitable loss function can make. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rubbish loss function \n",
    "def mnist_loss(predictions, targets): \n",
    "    predictions = predictions.softmax(dim=1)\n",
    "    return torch.where(targets==1, 1-predictions, predictions).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# superior loss function \n",
    "def cross_entropy_loss(predictions, targets):\n",
    "    predictions = predictions.softmax(dim=1)\n",
    "    cross = -(targets)*torch.log(predictions)\n",
    "    return cross.sum(1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test our loss function to see the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21.3209, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions = linear1(train_x[:4])\n",
    "test_targets = train_y[:4]\n",
    "cross_entropy_loss(test_predictions, test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a function to calculate our gradient for each weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad(xb, yb, model):\n",
    "    preds = model(xb)\n",
    "    loss = cross_entropy_loss(preds, yb)\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our training process:\n",
    "\n",
    "    - loop through our dataloader grabbing our batch\n",
    "    \n",
    "    - calculate the gradient for each weight\n",
    "    \n",
    "    - update our parameters\n",
    "    \n",
    "    - set our gradients to zero before the next loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, lr, params):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        for p in params:\n",
    "            p.data -= p.grad*lr\n",
    "            p.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to see how our accuracy changes with each epoch so we define a function to return accuracy on our validation set following each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(xb, yb):\n",
    "    preds = xb.softmax(dim=1).round()\n",
    "    correct = preds.max(1).indices == yb.max(1).indices\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model):\n",
    "    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n",
    "    return round(torch.stack(accs).mean().item(), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set our learning rate, pass weights and biases and calculate our initial accuracy following randomising weights: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7358"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial accuracy: \n",
    "lr = 1.\n",
    "params = weights,bias\n",
    "validate_epoch(linear1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train our network for one epoch and evaluate: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8099"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch(linear1, lr, params)\n",
    "validate_epoch(linear1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train for more epochs! We should see our model reaching over 86% accuracy which is significantly better than our baseline.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.884 0.8843 0.8836 0.8867 0.8846 0.8864 0.8883 0.8902 0.8908 0.8921 "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    train_epoch(linear1, lr, params)\n",
    "    print(validate_epoch(linear1), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However we don't have to handcode our loss function or one-hot encoding, PyTorch does a lot of this for us.\n",
    "Firstly instead of writing linear1 and init_params, we can do this in one step: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of linear1() and init\n",
    "linear_model = nn.Linear(28*28,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 784]), torch.Size([10]))"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b = linear_model.parameters()\n",
    "w.shape,b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimiser for optimising our parameters \n",
    "class BasicOptim:\n",
    "    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n",
    "    \n",
    "    # goes through each parameter making a change \n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params: p.data -= p.grad.data * self.lr\n",
    "# zeros each parameter out \n",
    "            # why do we need zero gradient again? \n",
    "# we don't want to calculate the gradient on our step once we've made it\n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params: p.grad = None\n",
    "            \n",
    "# exact same code we;ve seen before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in our parameters \n",
    "opt = BasicOptim(linear_model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0986"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_epoch(linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs):\n",
    "    for i in range(epochs):\n",
    "        train_epoch(model)\n",
    "        print(validate_epoch(model), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also see really nice results here too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.868 0.8815 0.8919 0.8944 0.8985 0.9024 0.8981 0.9023 0.9052 0.9057 "
     ]
    }
   ],
   "source": [
    "train_model(linear_model, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastai provides SGD class which does the same thing as BasicOptim: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8667 0.8807 0.8898 0.8946 0.8994 0.9014 0.9031 0.905 0.908 0.9061 "
     ]
    }
   ],
   "source": [
    "linear_model = nn.Linear(28*28,10)\n",
    "opt = SGD(linear_model.parameters(), lr)\n",
    "train_model(linear_model, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use Learner.fit instead of train_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C - train model using Learner instead of train_model\n",
    "learn = Learner(dls, nn.Linear(28*28,10), opt_func=SGD,\n",
    "                loss_func=cross_entropy_loss, metrics=batch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.570462</td>\n",
       "      <td>0.359681</td>\n",
       "      <td>0.863100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.406543</td>\n",
       "      <td>0.322185</td>\n",
       "      <td>0.882800</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.354019</td>\n",
       "      <td>0.316254</td>\n",
       "      <td>0.887600</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.327829</td>\n",
       "      <td>0.300225</td>\n",
       "      <td>0.897000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.314867</td>\n",
       "      <td>0.293585</td>\n",
       "      <td>0.898700</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.305436</td>\n",
       "      <td>0.293188</td>\n",
       "      <td>0.899400</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.299555</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.901900</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.294542</td>\n",
       "      <td>0.287618</td>\n",
       "      <td>0.901500</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.290914</td>\n",
       "      <td>0.283045</td>\n",
       "      <td>0.905100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.287434</td>\n",
       "      <td>0.282295</td>\n",
       "      <td>0.904600</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(10, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Nonlinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a really simple neural network we've created. It has one hidden layer and a softmax layer. Softmax is the non-linear layer that allows us to approximate any function. Key point is that if we are stacking linear functions, we can only approximate linear functions. If we include non-linear we can approximate any function! SGD is helping us find the right values for weights and bias that allow us to do this. \n",
    "\n",
    "And this is basically all there is to it! \n",
    "\n",
    "However we can use Pytorch and fastai to write out our NN in more neural net friendly terms. We previously had our softmax layer within our batch accuracy function and our cross entropy so we can remove it from them by writing new ones and including our softmax layer within our network definition: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>new_batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.591822</td>\n",
       "      <td>0.420661</td>\n",
       "      <td>0.835600</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.447905</td>\n",
       "      <td>0.363009</td>\n",
       "      <td>0.863400</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.389566</td>\n",
       "      <td>0.340810</td>\n",
       "      <td>0.874300</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.359758</td>\n",
       "      <td>0.324064</td>\n",
       "      <td>0.883400</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.342614</td>\n",
       "      <td>0.315903</td>\n",
       "      <td>0.887100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.328623</td>\n",
       "      <td>0.311107</td>\n",
       "      <td>0.890500</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.322100</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>0.894100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.314304</td>\n",
       "      <td>0.298794</td>\n",
       "      <td>0.895200</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.310156</td>\n",
       "      <td>0.293976</td>\n",
       "      <td>0.897800</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.305317</td>\n",
       "      <td>0.292644</td>\n",
       "      <td>0.898000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.301099</td>\n",
       "      <td>0.290675</td>\n",
       "      <td>0.899800</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.297970</td>\n",
       "      <td>0.288282</td>\n",
       "      <td>0.901000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.296289</td>\n",
       "      <td>0.285999</td>\n",
       "      <td>0.901500</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.294711</td>\n",
       "      <td>0.284485</td>\n",
       "      <td>0.903500</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.290232</td>\n",
       "      <td>0.283876</td>\n",
       "      <td>0.903200</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.289274</td>\n",
       "      <td>0.283095</td>\n",
       "      <td>0.903100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.289540</td>\n",
       "      <td>0.280504</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.284358</td>\n",
       "      <td>0.281753</td>\n",
       "      <td>0.904600</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.284358</td>\n",
       "      <td>0.278985</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.283954</td>\n",
       "      <td>0.281682</td>\n",
       "      <td>0.905700</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def new_cross_entropy_loss(predictions, targets):\n",
    "    cross = -(targets)*torch.log(predictions)\n",
    "    return cross.sum(1).mean()\n",
    "\n",
    "def new_batch_accuracy(xb, yb):\n",
    "    preds = xb.round()\n",
    "    correct = preds.max(1).indices == yb.max(1).indices\n",
    "    return correct.float().mean()\n",
    "\n",
    "simple_net = nn.Sequential(\n",
    "    nn.Linear(28*28,10),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "learn = Learner(dls, simple_net, opt_func=SGD,\n",
    "                loss_func=new_cross_entropy_loss, metrics=new_batch_accuracy)\n",
    "\n",
    "learn.fit(20,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(L(learn.recorder.values).itemgot(2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9057000279426575"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.recorder.values[-1][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following images are really facinating and almost have weight to them, like brail. It might be nice to import them into some CAD software and 3D print them to show the resulting landscapes. Create a series of landscape tiles. It could be like little scrabble tiles or wooden tiles for teaching the alphabet to children, the artefacts of the machine pre-school. Jeremy linked a nice technical paper on these images but I'm wondering if there's any papers on their cultural significance? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = learn.model\n",
    "\n",
    "# super cool function - we can look inside the network: \n",
    "w, b = m[0].parameters()\n",
    "\n",
    "# see what our different layers look like \n",
    "show_image(w[6].view(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the inbuilt CrossEntropyLoss however, this will require our different to be slighly different. Rather than providing one-hot encoded, the target should be in the form of a 1 dimentional tensor containing the expected value! So we will have to do some data re-processing first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_y = train_y.max(1).indices\n",
    "alt_valid_y = valid_y.max(1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_dset = list(zip(train_x,alt_train_y))\n",
    "alt_valid_dset = list(zip(valid_x,alt_valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_dl = DataLoader(alt_dset, batch_size=1024, shuffle=True)\n",
    "alt_valid_dl = DataLoader(alt_valid_dset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_dls = DataLoaders(alt_dl, alt_valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alt_batch_accuracy(xb, yb):\n",
    "    preds = xb.round().max(1).indices\n",
    "    correct = preds == yb\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_simple_net = nn.Sequential(\n",
    "    nn.Linear(28*28,10),\n",
    "    nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the Pytorch cross_entropy_loss function and we achieve similar results to before: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>alt_batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.836689</td>\n",
       "      <td>1.712604</td>\n",
       "      <td>0.774100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.709155</td>\n",
       "      <td>1.639656</td>\n",
       "      <td>0.851600</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.655310</td>\n",
       "      <td>1.616612</td>\n",
       "      <td>0.871900</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.628397</td>\n",
       "      <td>1.604066</td>\n",
       "      <td>0.878300</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.613231</td>\n",
       "      <td>1.595865</td>\n",
       "      <td>0.885400</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.603839</td>\n",
       "      <td>1.589970</td>\n",
       "      <td>0.889100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.597594</td>\n",
       "      <td>1.585335</td>\n",
       "      <td>0.894200</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.592174</td>\n",
       "      <td>1.581728</td>\n",
       "      <td>0.896400</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.588298</td>\n",
       "      <td>1.578578</td>\n",
       "      <td>0.898100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.585384</td>\n",
       "      <td>1.576061</td>\n",
       "      <td>0.899400</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.582455</td>\n",
       "      <td>1.573753</td>\n",
       "      <td>0.901500</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.580006</td>\n",
       "      <td>1.571982</td>\n",
       "      <td>0.902600</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.577526</td>\n",
       "      <td>1.570448</td>\n",
       "      <td>0.902900</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.575424</td>\n",
       "      <td>1.568796</td>\n",
       "      <td>0.904500</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.574370</td>\n",
       "      <td>1.567338</td>\n",
       "      <td>0.905900</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.572932</td>\n",
       "      <td>1.566501</td>\n",
       "      <td>0.906800</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.572228</td>\n",
       "      <td>1.565136</td>\n",
       "      <td>0.908400</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.570152</td>\n",
       "      <td>1.564233</td>\n",
       "      <td>0.909300</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.569296</td>\n",
       "      <td>1.563104</td>\n",
       "      <td>0.909800</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.568373</td>\n",
       "      <td>1.562371</td>\n",
       "      <td>0.910700</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alt_learn = Learner(alt_dls, alt_simple_net, opt_func=SGD,\n",
    "                loss_func=F.cross_entropy, metrics=alt_batch_accuracy)\n",
    "alt_learn.fit(20,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final aside, there are other ways of writing cross entropy loss. We can test these on our first batch: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "xv, yv = first(alt_valid_dl)\n",
    "predictions = alt_simple_net(xv)\n",
    "labels = yv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5690, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = F.cross_entropy\n",
    "criterion(input=predictions, target=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5690, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion2 = nn.NLLLoss(reduction='mean')\n",
    "criterion2(torch.log(predictions.softmax(dim=1)), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5690, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion2(F.log_softmax(predictions, dim=1), labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even use our old definition of entropy loss if we one-hot encode our target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5690, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot = torch.nn.functional.one_hot(labels, num_classes=10)\n",
    "cross_entropy_loss(predictions, one_hot)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
